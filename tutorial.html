<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.6"/>
<title>Padenti: Tutorial</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Padenti
   &#160;<span id="projectnumber">0.2</span>
   </div>
   <div id="projectbrief">An OpenCL-accelerated Random Forests implementation for Computer Vision applications using local features</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.6 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Classes</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Variables</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Pages</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="index.html">index</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Tutorial </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>In this brief step-by-step tutorial we will explain how to use the Padenti library to implement a naive hand segmenter.</p>
<h1><a class="anchor" id="dataset"></a>
Downloading and processing the dataset</h1>
<p>The input data consists in the depthmaps stream captured by a RGB-D camera. A depthmap is a two dimensional image where pixel data represents the distance (usually in mm) from the camera sensor at that specific pixel location. In this tutorial we are going to use the NYU Hand pose dataset and, more specifically, its subset provided for hand segmentation. The dataset can be downloaded at the following <a href="http://cims.nyu.edu/~tompson/NYU_Hand_Pose_Dataset.htm#download">link</a> (<a href="https://drive.google.com/open?id=0B_ILMW63e765TFJDQm0yQTRHcFE">mirror</a>).</p>
<p>Before being used, the dataset must converted in a format compatible with the Padenti library. For this purpose, we can use the process_dataset.py python script shipped with the library (it can be found in the script subfolder). Once the dataset has been downloaded and extracted, we can excute the script as follows</p>
<div class="fragment"><div class="line">python process_dataset.py DATASET_PATH OUT_PATH</div>
</div><!-- fragment --><p>where DATASET_PATH is the directory containing the original dataset images, while OUT_PATH is the processed dataset destination path. After the script execution ends, OUT_PATH contains two folders (train and test) where the processed images of the training set and the test set are stored.</p>
<dl class="section note"><dt>Note</dt><dd>The script above requires the numpy and OpenCV2 python wrappers to be installed. The Windows installers can be found <a href="http://www.lfd.uci.edu/~gohlke/pythonlibs">here</a></dd></dl>
<p>For each image of the original dataset, two new images are generated: a 16bit single channel image &lt;IMAGE&gt;_depth.png containing the depth values, and a RGB image representing the depthmap labelling. Labels mark the depthmap pixels as hand (red color) or background (blue colors). Pixels that must not be processed (in this case, pixels whose depth values is greater than 2m) are labeles with black. A labels image example is shown below</p>
<div class="image">
<img src="labels_example.png" alt="labels_example.png"/>
</div>
 <dl class="section note"><dt>Note</dt><dd>The test set is built by randomly leaving 10% of the images out of the orginal dataset.</dd></dl>
<h1><a class="anchor" id="feature"></a>
Defining a feature</h1>
<p>Features are at the heart of the learning problem. In the case of the Padenti library, we tackle computer vision problems where features are defined locally on a per-pixel basis. This means that features are rather simple and usally work on the value stored at the current pixels (or at a subset of neighbouring pixels).</p>
<p>To generalize with respect to arbitrary features, Padenti provides to the user some facilities to define custom features. We define a standard OpenCL C function prototype that must be implemented within a feature.cl file and passed to the library. The function prototype is the following</p>
<div class="fragment"><div class="line">feat_t computeFeature(__read_only image_t image,</div>
<div class="line">                      uint nChannels, uint width, uint height, int2 coords,</div>
<div class="line">                      __global <span class="keywordtype">int</span> *treeLeftChildren,</div>
<div class="line">                      __global <span class="keywordtype">float</span> *treePosteriors,</div>
<div class="line">                      __read_only image2d_t imageNodesID,</div>
<div class="line">                      __local feat_t *features, uint featDim)</div>
<div class="line">  {....}</div>
</div><!-- fragment --><p>image can be a bidimensional image2d_t object with up to 4 channels or an image3d_t object with single-value channels and an arbitrary number of layers. nChannels, width and height specify the number of channels (or layers for 3D images) and the width and height of each image channel (or layer). coords stores the 2D coordinates of the currently processed pixel. treeLeftChildren and treePosteriors are the values of the left children index and per-class posterior probabilities for all the nodes of the currently used tree, stored consecutively in a breadth-first fashion (i.e. sorted by node index). Finally, the features pointer stores the features values, while featDim indicates the feature dimension (i.e. number of entries in the feature vector).</p>
<p>Returning to the original problem, previous works in the computer vision field proposed different features for body-parts segmentation, some of which can be applied to hand segmentation as well. Here we consider a simplified version of the features applied in <a href="http://research.microsoft.com/en-us/projects/vrkinect/">this work</a> and defined as</p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ f_{\mathbf{u}}(\mathcal{D},\mathbf{x}) = \mathcal{D}(\mathbf{x}) - \frac{\mathcal{D}(\mathbf{x}+\mathbf{u})}{D(\mathbf{x})}*d \]" src="form_0.png"/>
</p>
<p>where <img class="formulaInl" alt="$\mathcal{D}$" src="form_1.png"/> is the depthmap image, <img class="formulaInl" alt="$\mathbf{x}$" src="form_2.png"/> are the current pixel coordinates, <img class="formulaInl" alt="$\mathbf{u}$" src="form_3.png"/> is an offset vector and <img class="formulaInl" alt="$d$" src="form_4.png"/> is a reference depth value. This feature returns strong a response in the presence of depth discontinuities. Combined with the Random Forests framework, the feature is able to discrimate with success among body parts. In this tutorial we will use the same feature to discrimante between hand pixels and background pixels.</p>
<p>The OpenCL C feature implementaion using the above feature is shown below:</p>
<div class="fragment"><div class="line"><span class="comment">// The feat_type.cl file is generated at run-time and store a &quot;feat_t&quot; typedef</span></div>
<div class="line"><span class="comment">// definition where the feature items value and response type is defined.</span></div>
<div class="line"><span class="preprocessor">#include &lt;feat_type.cl&gt;</span></div>
<div class="line"></div>
<div class="line"></div>
<div class="line"><span class="preprocessor">#define TARGET_DEPTH (500.0f)</span></div>
<div class="line"><span class="preprocessor"></span><span class="preprocessor">#define BG_RESPONSE (10000.0f)</span></div>
<div class="line"><span class="preprocessor"></span></div>
<div class="line"></div>
<div class="line">feat_t computeFeature(__read_only image2d_t image,</div>
<div class="line">              uint nChannels, uint width, uint height, int2 coords,</div>
<div class="line">              __global <span class="keywordtype">int</span> *treeLeftChildren,</div>
<div class="line">              __global <span class="keywordtype">float</span> *treePosteriors,</div>
<div class="line">              __read_only image2d_t imageNodesID,</div>
<div class="line">              __local feat_t *features, uint featDim)</div>
<div class="line">{</div>
<div class="line">  <span class="keyword">const</span> sampler_t sampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST;</div>
<div class="line">  feat_t depth, response;</div>
<div class="line">  int2 offset;</div>
<div class="line">  <span class="keywordtype">bool</span> outOfBorder;</div>
<div class="line"></div>
<div class="line">  <span class="comment">// Read the depth value at current pixel</span></div>
<div class="line">  depth = (feat_t)read_imageui(image, sampler, coords).x;</div>
<div class="line">  response = depth;</div>
<div class="line"></div>
<div class="line">  <span class="comment">// Compute the dispacement vector with respect to current coordinates</span></div>
<div class="line">  <span class="comment">// Note: to access feature values we always use the ACCESS_FEATURE macro, where the</span></div>
<div class="line">  <span class="comment">// first value is the features pointer, the second value is the item index and the</span></div>
<div class="line">  <span class="comment">// last value is the feature dimension</span></div>
<div class="line">  offset.x = </div>
<div class="line">    coords.x+(int)round((<span class="keywordtype">float</span>)ACCESS_FEATURE(features, 0, featDim)*TARGET_DEPTH/depth);</div>
<div class="line">  offset.y =</div>
<div class="line">    coords.y+(int)round((<span class="keywordtype">float</span>)ACCESS_FEATURE(features, 1, featDim)*TARGET_DEPTH/depth);</div>
<div class="line">  </div>
<div class="line">  <span class="comment">// Check if the displacement vector is beyond the image borders. In that case,</span></div>
<div class="line">  <span class="comment">// the depth value is set to BG_RESPONSE</span></div>
<div class="line">  outOfBorder = offset.x&lt;0 || offset.x&gt;=width || offset.y&lt;0 || offset.y&gt;=height;</div>
<div class="line">  depth = (outOfBorder) ? (feat_t)BG_RESPONSE : (feat_t)read_imageui(image, sampler, offset).x;</div>
<div class="line"></div>
<div class="line">  <span class="comment">// Finally, return the feature response, defined as the difference between the depth values</span></div>
<div class="line">  <span class="comment">// at the pixel coords and at the neighbour pixel</span></div>
<div class="line">  response -= (depth) ? depth : (feat_t)BG_RESPONSE;</div>
<div class="line"></div>
<div class="line">  <span class="keywordflow">return</span> response;</div>
<div class="line"> }</div>
</div><!-- fragment --><p>Please look at the comments for futher details about features implementation.</p>
<dl class="section note"><dt>Note</dt><dd>The implementation of custom features requires a minimum knowledge of the OpenCL C language and image objects processing within OpenCL kernels.</dd></dl>
<h1><a class="anchor" id="train"></a>
Training</h1>
<p>The test_tree_trainer.cpp file in the test subfolder provides the code for training a single Random Forest tree. In this section we will walk through the basic classes that the Padenti library makes available to load a training set and train multiple trees of a random forests ensemble.</p>
<p>The Padenti library uses C++ templates to generalize the Random Forests implementation with respect to both input images type (pixel type and number of channels) and features type (number and type of entries). Here we consider single channel 16bit depthmaps. We thus start with the definition of the classes responsible of training set loading</p>
<div class="fragment"><div class="line"><span class="keyword">typedef</span> <a class="code" href="class_c_v_image_loader.html">CVImageLoader&lt;unsigned short, 1&gt;</a> DepthmapLoaderT;</div>
<div class="line"><span class="keyword">typedef</span> <a class="code" href="class_c_v_r_g_b_labels_loader.html">CVRGBLabelsLoader</a> LabelsLoaderT;</div>
<div class="line"><span class="keyword">typedef</span> <a class="code" href="class_uniform_image_sampler.html">UniformImageSampler&lt;unsigned short, 1&gt;</a> SamplerT;</div>
<div class="line"><span class="keyword">typedef</span> <a class="code" href="class_training_set.html">TrainingSet&lt;unsigned short, 1&gt;</a> TrainingSetT;</div>
</div><!-- fragment --><p>The <a class="el" href="class_c_v_image_loader.html">CVImageLoader</a> is a generic class responsible of loading an image from disk. We are going to use this class to load deapthmaps, so we specialize the template to work with single channel images with unsigned short values. The <a class="el" href="class_c_v_r_g_b_labels_loader.html">CVRGBLabelsLoader</a> class is similar to the <a class="el" href="class_c_v_image_loader.html">CVImageLoader</a>, but it works specifically with RGB images, so we do not need template specialization. The class will be used class to read labels images from disk. Finally, both the class for depthmaps sampling (<a class="el" href="class_uniform_image_sampler.html">UniformImageSampler</a>) and training set handling (<a class="el" href="class_training_set.html" title="TrainingSet class. The class is responsible of loading and sampling a set of images pairs stored on d...">TrainingSet</a>) must be specialized with the depthmaps pixel type and number of channels.</p>
<p>Templates are also used to generalize the Random Forests implementation with respect to feature type and size. Since the feature is defined by the 2D offset <img class="formulaInl" alt="$\mathbf{u}$" src="form_3.png"/> with respect to current pixel location <img class="formulaInl" alt="$\mathbf{x}$" src="form_2.png"/>, we represent it as a 2D short int vector and declare the template specialization for the tree and trainer classes as follows</p>
<div class="fragment"><div class="line"><span class="keyword">typedef</span> <a class="code" href="class_tree.html">Tree&lt;short int, 2, N_LABELS&gt;</a> TreeT;</div>
<div class="line"><span class="keyword">typedef</span> <a class="code" href="class_tree_trainer_parameters.html">TreeTrainerParameters&lt;short int, 2&gt;</a> TreeTrainerParametersT;</div>
<div class="line"><span class="keyword">typedef</span> <a class="code" href="class_c_l_tree_trainer.html">CLTreeTrainer&lt;unsigned short, 1, short int, 2, N_LABELS&gt;</a> TreeTrainerT;</div>
</div><!-- fragment --><p>the names of classes are self explanatory (please refer to the documentation for additional details).</p>
<p>Once the templates specialization has been defined, we can instantiate the corresponding classes.</p>
<div class="fragment"><div class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">char</span> RGB2LABEL[][3] ={</div>
<div class="line">  {255, 0, 0},      <span class="comment">// hand</span></div>
<div class="line">  {0, 0, 255},    <span class="comment">// body</span></div>
<div class="line">};</div>
<div class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keywordtype">size_t</span> N_LABELS = <span class="keyword">sizeof</span>(RGB2LABEL)/(<span class="keyword">sizeof</span>(<span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>)*3);</div>
<div class="line"></div>
<div class="line">DepthmapLoaderT depthmapLoader;</div>
<div class="line">LabelsLoaderT labelsLoader(RGB2LABEL, N_LABELS);</div>
<div class="line">SamplerT sampler(N_SAMPLES);</div>
<div class="line">TrainingSetT trainingSet(trainingSetPath, <span class="stringliteral">&quot;_depth.png&quot;</span>, <span class="stringliteral">&quot;_labels.png&quot;</span>, N_LABELS,</div>
<div class="line">                         depthmapLoader, labelsLoader, sampler);</div>
</div><!-- fragment --><p>depthmapLoader and labelsLoader are responsible of depthmap/labels loading. The latter requires a Nx3 matrix specifying the RGB values associated to each class label (defined by the RGB2LABEL matrix in the code above). sampler executes the depthmap sampling and, for each depthmap, extracts up to N_SAMPLES. trainingSet loads the training set depthmap/labels pairs stored in the path specified by trainingSetPath and employs the depthmapLoader/labelsLoader for images pairs loading and sampler for sampling.</p>
<div class="fragment"><div class="line">TreeT tree(treeID, TREE_DEPTH);</div>
<div class="line">TreeTrainerT trainer(feature_clPath);</div>
</div><!-- fragment --><p>Next, we create a TreeT object tree, identified univocally by the treeID index and with depth TREE_DEPTH. The trainer object is responsible of training the TreeT object on the loaded training set. The first parameter specifies the directory where the previously defined file feature.cl containing the feature implementation is stored.</p>
<p>Before launching the training, we must specify the training parameters. Here we suppose that we extract 2000 pixels at each pixel and test 20 thresholds for each feature. We bound the features offset within the range ]0,60[ and the tresholds within ]-200,200[. Finally, we stop the training of a node if less than 300 pixels reach that node. We can specify these parameters filling the corresponding fields of a TreeTrainerParametersT structure</p>
<div class="fragment"><div class="line">TreeTrainerParametersT params;</div>
<div class="line">params.nFeatures = 2000;</div>
<div class="line">params.nThresholds = 20;</div>
<div class="line">params.featLowBounds[0] = -60;</div>
<div class="line">params.featLowBounds[1] = -60;</div>
<div class="line">params.featUpBounds[0] = 60;</div>
<div class="line">params.featUpBounds[1] = 60;</div>
<div class="line">params.thrLowBound = -200;</div>
<div class="line">params.thrUpBound = 200;</div>
<div class="line">params.perLeafSamplesThr = 300;</div>
</div><!-- fragment --><p>We can now launch the training</p>
<div class="fragment"><div class="line">trainer.train(tree, trainingSet, params, 1, TRAIN_DEPTH);</div>
</div><!-- fragment --><p>Once the training ends, we can save the tree to disk using the tree save method</p>
<div class="fragment"><div class="line">tree.save(treePath);</div>
</div><!-- fragment --><h1><a class="anchor" id="test"></a>
Test</h1>
<p>After one or more tree has been trained, we can use the Padenti library to predict which pixels belong to the hand for an unseen input depthmap. Similarly to the previous section, we will go through the steps needed for loading a trained forest and performing hand segmentation using the Padenti library. The output of the test is a two layers float image where the first layer stores the probability <img class="formulaInl" alt="$P(hand|\mathbf{x})$" src="form_5.png"/> for each pixel <img class="formulaInl" alt="$\mathbf{x}$" src="form_2.png"/>, i.e. the predicted probability for that pixel of being a hand pixel, whereas the second layers stores the <img class="formulaInl" alt="$P(background|\mathbf{x})$" src="form_6.png"/> posterior probability (i.e. the probability of being a background pixel).</p>
<p>We start by specializing the template classes needed for tree/depthmap loading and classification</p>
<div class="fragment"><div class="line"><span class="keyword">typedef</span> <a class="code" href="class_tree.html">Tree&lt;short int, 2, N_LABELS&gt;</a> TreeT;</div>
<div class="line"><span class="keyword">typedef</span> <a class="code" href="class_image.html">Image&lt;unsigned short, 1&gt;</a> DepthT;</div>
<div class="line"><span class="keyword">typedef</span> <a class="code" href="class_image.html">Image&lt;unsigned char, 1&gt;</a> MaskT;</div>
<div class="line"><span class="keyword">typedef</span> <a class="code" href="class_image.html">Image&lt;float, N_LABELS&gt;</a> PredictionT;</div>
<div class="line"><span class="keyword">typedef</span> <a class="code" href="class_c_v_image_loader.html">CVImageLoader&lt;unsigned short, 1&gt;</a> ImageLoaderT;</div>
<div class="line"><span class="keyword">typedef</span> <a class="code" href="class_c_l_classifier.html">CLClassifier&lt;unsigned short, 1, short, 2, N_LABELS&gt;</a> ClassifierT;</div>
</div><!-- fragment --><p>The <a class="el" href="class_image.html" title="Base class to store an image. The class acts as a simple container of pixels values. Arbitrary pixels type and number of channels are supported through template parameters. Image pixels must be stored continuously in memory. ">Image</a> class defines a generic container for image pixels, while the <a class="el" href="class_c_l_classifier.html">CLClassifier</a> implement the Random Forests classification. We can now load the trees from disk and pass them to the classifier object</p>
<div class="fragment"><div class="line">TreeT trees[nTrees];</div>
<div class="line">ClassifierT classifier(feature_clPath);</div>
<div class="line"></div>
<div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">int</span> t=0; t&lt;nTrees; t++)</div>
<div class="line">{</div>
<div class="line">  trees[t].load(treesPath[t], t);</div>
<div class="line">  classifier &lt;&lt; trees[t];</div>
<div class="line">}</div>
</div><!-- fragment --><p>As in the training case, the classifier needs the path of the directory where the feature implementation is stored. Random Forests trees can be easily loaded into the classifier using the left shift operator.</p>
<p>We can now load the input depthmap. Since not all depthmpap pixels need to be processed, we use the OpenCV library to create a binary mask where only pixels whose depth value is different from zeros are selected</p>
<div class="fragment"><div class="line"><span class="comment">// Load depth </span></div>
<div class="line">ImageLoaderT imageLoader;</div>
<div class="line">DepthT depthmap = imageLoader.load(depthmapPath);</div>
<div class="line"></div>
<div class="line"><span class="comment">// Create a mask</span></div>
<div class="line">MaskT mask(depthmap.getWidth(), depthmap.getHeight());</div>
<div class="line"></div>
<div class="line"><span class="comment">// Wrap Padenti image objects within an OpenCV matrix ...</span></div>
<div class="line">cv::Mat cvDepth(depthmap.getHeight(), depthmap.getWidth(), CV_16U,</div>
<div class="line">        <span class="keyword">reinterpret_cast&lt;</span><span class="keywordtype">unsigned</span> <span class="keywordtype">char</span>*<span class="keyword">&gt;</span>(depthmap.getData()));</div>
<div class="line">cv::Mat cvMask(cvDepth.rows, cvDepth.cols, CV_8U,</div>
<div class="line">       reinterpret_cast&lt;unsigned char*&gt;(mask.getData()));</div>
<div class="line"></div>
<div class="line"><span class="comment">// ... and use OpenCV function to fill the mask</span></div>
<div class="line">cvMask.setTo(0);</div>
<div class="line">cvMask.setTo(1, cvDepth&gt;0);</div>
</div><!-- fragment --><p>Once the depthmap and mask objects have been initialized, we can perform prediction</p>
<div class="fragment"><div class="line">PredictionT prediction(cvDepth.cols, cvDepth.rows);</div>
<div class="line">classifier.predict(depthmap, prediction, mask);</div>
</div><!-- fragment --><p>The prediction image will contain the classification result. Using the OpenCV library we can wrap each layer into a 2D float image and show it</p>
<div class="fragment"><div class="line">cv::Mat cvHand(cvDepth.rows, cvDepth.cols, CV_32F,</div>
<div class="line">               reinterpret_cast&lt;void*&gt;(prediction.getData()));</div>
<div class="line">cv::Mat cvBackground(cvDepth.rows, cvDepth.cols, CV_32F,</div>
<div class="line">                     reinterpret_cast&lt;void*&gt;(prediction.getData()+cvDepth.rows*cvDepth.cols));</div>
<div class="line"></div>
<div class="line">cv::imshow(<span class="stringliteral">&quot;hand&quot;</span>, cvHand);</div>
<div class="line">cv::imshow(<span class="stringliteral">&quot;background&quot;</span>, cvBackground);</div>
</div><!-- fragment --><p>A prediction result example is shown below</p>
<div class="image">
<img src="hand_seg_result.png" alt="hand_seg_result.png"/>
</div>
<p> While the per-pixel segmentation is still not perfect, it provides a strong signal that can be used as the input of another algorithm (i.e. camshift) to easily track the hand position within the image. </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Wed Aug 31 2016 14:25:15 for Padenti by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.6
</small></address>
</body>
</html>
